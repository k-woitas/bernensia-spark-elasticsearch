{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c5fa12",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>E-rara data transformation with Spark and ingest to Elasticsearch<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Processing-e-rara-data\" data-toc-modified-id=\"Processing-e-rara-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Processing e-rara data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-from-AWS-S3\" data-toc-modified-id=\"Load-from-AWS-S3-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load from AWS S3</a></span></li><li><span><a href=\"#Get-to-know-the-data\" data-toc-modified-id=\"Get-to-know-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Get to know the data</a></span></li><li><span><a href=\"#Cleaning-and-formatting-the-data\" data-toc-modified-id=\"Cleaning-and-formatting-the-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Cleaning and formatting the data</a></span></li><li><span><a href=\"#Text-processing:-Named-entity-recognition\" data-toc-modified-id=\"Text-processing:-Named-entity-recognition-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Text processing: Named entity recognition</a></span></li></ul></li><li><span><a href=\"#Ingest-to-Elasticsearch\" data-toc-modified-id=\"Ingest-to-Elasticsearch-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Ingest to Elasticsearch</a></span></li><li><span><a href=\"#Query-Elasticsearch\" data-toc-modified-id=\"Query-Elasticsearch-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Query Elasticsearch</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d403e78",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761c310e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:46:45.519234Z",
     "start_time": "2021-10-04T06:46:45.512317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported necessary libraries.\n"
     ]
    }
   ],
   "source": [
    "# Import basic libraries\n",
    "import os, re, json\n",
    "import pandas as pd\n",
    "print('Successfully imported necessary libraries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3810a11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:45:55.410273Z",
     "start_time": "2021-10-04T06:45:55.407095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (1.18.57)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from boto3) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.57 in /opt/conda/lib/python3.8/site-packages (from boto3) (1.21.57)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.57->boto3) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.57->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.57->boto3) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# for I/O AWS S3\n",
    "!pip install boto3\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e488b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:45:50.309860Z",
     "start_time": "2021-10-04T06:45:50.302680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the required Python dependencies\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# modules for handling dataframes\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, split, lit, when, regexp_replace, concat\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, IntegerType, StringType\n",
    "\n",
    "# for user defined functions\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bd6a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:48:02.692735Z",
     "start_time": "2021-10-04T06:48:02.502592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Session configuration\n",
    "#conf.setMaster(\"spark://spark-master:7077\")\n",
    "conf.setMaster(\"local[2]\")\n",
    "\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.cores\", \"1\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.core.connection.ack.wait.timeout\", \"1200\")\n",
    "\n",
    "# Elasticsearch\n",
    "conf.set(\"spark.executor.extraClassPath\", \"elasticsearch-hadoop-7.12.0/dist/elasticsearch-hadoop-20_2.11-7.12.0.jar, \\\n",
    "            elasticsearch-hadoop-7.12.0/dist/elasticsearch-spark-20_2.11-7.12.0.jar\")\n",
    "conf.set(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0\")\n",
    "conf.set(\"es.index.auto.create\", \"true\")\n",
    "\n",
    "# Initialize a Spark session with configuration\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('elastic') \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Instantiate a Spark Context\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6a1401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:48:30.065210Z",
     "start_time": "2021-10-04T06:48:30.056408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>elastic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0a05f680d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56e42d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  stop Spark Session, if necessary\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb5f7af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T06:50:17.063277Z",
     "start_time": "2021-10-04T06:50:16.877179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python on driver: /opt/conda/bin/python\n",
      "Python on nodes:  ['/opt/conda/bin/python']\n"
     ]
    }
   ],
   "source": [
    "# check executable binaries for the Python interpreter\n",
    "import sys\n",
    "print(\"Python on driver: \" + sys.executable)\n",
    "distData = sc.parallelize(range(100))\n",
    "python_distros = distData.map(\n",
    "    lambda x: sys.executable).distinct().collect()\n",
    "print(\"Python on nodes: \", python_distros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda9bc2",
   "metadata": {},
   "source": [
    "# Processing e-rara data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332655a4",
   "metadata": {},
   "source": [
    "## Load from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca33ac75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T07:26:43.407291Z",
     "start_time": "2021-10-04T07:26:42.838443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bernensia-json-fulltext/10179500.json',\n",
       " 'bernensia-json-fulltext/10347968.json',\n",
       " 'bernensia-json-fulltext/10381638.json',\n",
       " 'bernensia-json-fulltext/10722710.json',\n",
       " 'bernensia-json-fulltext/10733431.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of, and keys resp. filepaths of objects in AWS S3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_objects(Bucket='bgd-content', MaxKeys=1000, \\\n",
    "                           Prefix='bernensia-json-fulltext/')\n",
    "#response['ResponseMetadata']\n",
    "keys = []\n",
    "file_paths = []\n",
    "for i in response['Contents'][1:]:\n",
    "    keys.append(i['Key'])\n",
    "    file_paths.append('s3a://bgd-content/' + i['Key'])\n",
    "print(len(keys))\n",
    "keys[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da578c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T07:33:26.602795Z",
     "start_time": "2021-10-04T07:33:26.594541Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_json(path, bigfile):\n",
    "    '''\n",
    "    Binds the JSON files of a directy together in one big JSON file.\n",
    "    \n",
    "    :param path: path to directory with the JSON files which shall be integrated\n",
    "    :param bigfile: name of the resulting big JSON file\n",
    "    '''\n",
    "    result = list()\n",
    "    files = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    for f in files:\n",
    "        with open(path + f, 'r') as infile:\n",
    "            result.append(json.load(infile))\n",
    "    with open(bigfile, 'w') as output_file:\n",
    "        json.dump(result, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43942c1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T07:34:38.044568Z",
     "start_time": "2021-10-04T07:33:29.426889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributor: string (nullable = true)\n",
      " |-- coverage: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- fulltext: string (nullable = true)\n",
      " |-- id_intern: string (nullable = true)\n",
      " |-- identifier: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- rights: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 example URI 's3a://bgd-content/bernensia-json-fulltext/15251979.json'\n",
    "# AWS S3 example URI 's3a://bgd-content/eperiodica-json-fulltext/zgh-001:1941:3::313.json'\n",
    "\n",
    "# make local directory for downloaded JSON files\n",
    "\n",
    "os.makedirs('download/bernensia-json-fulltext/')\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    s3.download_file('bgd-content', keys[i], 'download/{}'.format(keys[i]))\n",
    "\n",
    "merge_json('download/bernensia-json-fulltext/', 'erara_big.json')\n",
    "\n",
    "# load big JSON file into Spark DataFrame\n",
    "df = spark.read.json('erara_big.json',  multiLine=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de67d6f7",
   "metadata": {},
   "source": [
    "## Get to know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b0bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " contributor | null                 \n",
      " coverage    | Bern                 \n",
      " creator     | [s.n.]               \n",
      " date        | [\"1812\",\"1824\"]      \n",
      " description | [\"EnthÃ¤lt Druckb... \n",
      " format      | 1 Mappe (10 Druck... \n",
      " fulltext    | null                 \n",
      " id_intern   | 20150342             \n",
      " identifier  | [doi:10.3931/e-ra... \n",
      " language    | ger                  \n",
      " publisher   | [Haller]             \n",
      " relation    | vignette : https:... \n",
      " rights      | pdm                  \n",
      " source      | null                 \n",
      " subject     | Kaffeehandel         \n",
      " title       | [Kaffee-Verpackun... \n",
      " type        | [Text, Book]         \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)  # show the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "455ab9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----+\n",
      "|type                                           |count|\n",
      "+-----------------------------------------------+-----+\n",
      "|[Text, Book]                                   |472  |\n",
      "|[Image, Illustrated Material]                  |80   |\n",
      "|[Image, Map]                                   |9    |\n",
      "|[Text, Zeitschrift]                            |4    |\n",
      "|[Text, Periodical, Zeitschrift]                |3    |\n",
      "|[Text, Periodical, Zeitschrift, [PÃ©riodiques]]|1    |\n",
      "|[Other, Music Print]                           |1    |\n",
      "|[Text, Monografische Reihe]                    |1    |\n",
      "+-----------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"type\").count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452841a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "# Check for generally empty fields\n",
    "possibly_empty = ['source', 'contributor']\n",
    "for field in possibly_empty:\n",
    "    print(df.where(df[field] != \"null\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e02dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|rights|count|\n",
      "+------+-----+\n",
      "|pdm   |571  |\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"rights\").count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25252d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|subject                                  |count|\n",
      "+-----------------------------------------+-----+\n",
      "|null                                     |435  |\n",
      "|Schausteller                             |19   |\n",
      "|Wasserbau                                |10   |\n",
      "|Menagerie                                |7    |\n",
      "|Zirkus                                   |6    |\n",
      "|[\"Artist\",\"KnieFamilie\"]                 |6    |\n",
      "|Museumsgesellschaft                      |6    |\n",
      "|[\"Schausteller\",\"Wachsfigurenkabinett\"]  |5    |\n",
      "|JuragewÃ¤sserkorrektion                  |3    |\n",
      "|Artist                                   |3    |\n",
      "|Villmergerkrieg (1712)                   |3    |\n",
      "|[\"Elefanten\",\"Menagerie\"]                |3    |\n",
      "|[\"Ringen\",\"Schausteller\"]                |2    |\n",
      "|Volkswirtschaft                          |2    |\n",
      "|Ballonfahrt                              |2    |\n",
      "|[\"Mechanisches Kunstwerk\",\"Schausteller\"]|2    |\n",
      "|Panorama                                 |2    |\n",
      "|Recht                                    |2    |\n",
      "|[\"Finanzpolitik\",\"Ã–ffentliche Finanzen\"]|2    |\n",
      "|Hochverratsprozess                       |2    |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"subject\").count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c323b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----+\n",
      "|coverage                            |count|\n",
      "+------------------------------------+-----+\n",
      "|Bern                                |312  |\n",
      "|null                                |151  |\n",
      "|949.4                               |21   |\n",
      "|[\"Bern\",\"949.4\"]                    |8    |\n",
      "|330                                 |7    |\n",
      "|550                                 |6    |\n",
      "|320                                 |5    |\n",
      "|624                                 |3    |\n",
      "|Basel                               |3    |\n",
      "|[\"Bern (Kanton)\",\"949.4\"]           |3    |\n",
      "|[\"090\",\"020\"]                       |3    |\n",
      "|[\"Bern (Kanton)\",\"Geschichte\",\"340\"]|2    |\n",
      "|Geschichte                          |2    |\n",
      "|340                                 |2    |\n",
      "|[\"949.4\",\"060\"]                     |2    |\n",
      "|Bern (Kanton)                       |2    |\n",
      "|300                                 |1    |\n",
      "|390                                 |1    |\n",
      "|[\"Geschichte\",\"949.4\"]              |1    |\n",
      "|[\"Geschichte\",\"390\",\"949.4\"]        |1    |\n",
      "+------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"coverage\").count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1b9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|language|count|\n",
      "+--------+-----+\n",
      "|ger     |478  |\n",
      "|fre     |90   |\n",
      "|lat     |3    |\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"language\").count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8908d791",
   "metadata": {},
   "source": [
    "## Cleaning and formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f037b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributor: string (nullable = true)\n",
      " |-- coverage: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- fulltext: string (nullable = true)\n",
      " |-- id_intern: string (nullable = true)\n",
      " |-- identifier: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- rights: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- source_collection: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean 'type' field and rename 'type' to prevent confusion in Elasticsearch use\n",
    "# Not yet in use\n",
    "'''\n",
    "df_type_1 = df.withColumn('type_new_1', df['type'][0].cast('String'))\n",
    "df_type_2 = df_type_1.withColumn('type_new_2', df['type'][1].cast('String')).replace(\"Zeitschrift\", \"Periodical\") \\\n",
    "                    .replace(\"Monografische Reihe\", \"Monograph Series\").drop(test['type'])\n",
    "df_type_3 = df_type_2.withColumn('document_type', concat(lit('['), 'type_new_1', lit(', '), 'type_new_2', lit(']'))) \\\n",
    "                .drop(df_type_2['type_new_1']).drop(df_type_2['type_new_2'])\n",
    "'''\n",
    "# Add field for source collection\n",
    "df_coll = df.withColumn(\"source_collection\", lit('E-Rara'))\n",
    "\n",
    "# Change language codes into readable ones\n",
    "df_cleaned = df_coll.withColumn('language', df_coll['language'].cast('String')) \\\n",
    "                    .replace(\"ger\", \"German\") \\\n",
    "                    .replace(\"fre\", \"French\") \\\n",
    "                    .replace(\"lat\", \"Latin\")\n",
    "\n",
    "df_cleaned.persist().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde2b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make German and 'other' language subsets\n",
    "df_cleaned_german = df_cleaned.filter(df_cleaned.language == \"German\").persist()\n",
    "#df_cleaned_other = df_cleaned.filter(df_cleaned.language != \"German\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16196ef2",
   "metadata": {},
   "source": [
    "## Text processing: Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99bafb8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T08:57:36.535411Z",
     "start_time": "2021-10-04T08:57:34.100602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install SpaCy\n",
    "!pip install -U spacy==3.1       # to force the latest version\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478210fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T08:57:50.905054Z",
     "start_time": "2021-10-04T08:57:50.900306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download spaCy large German language model, 571.2 MB\n",
    "!python -m spacy download de_core_news_lg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2177fcea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T08:58:01.120189Z",
     "start_time": "2021-10-04T08:57:57.502119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy language model implemented successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the language model\n",
    "import de_core_news_lg\n",
    "nlp = de_core_news_lg.load()              # alternative if problems occur: nlp = spacy.load('de_core_news_lg')     \n",
    "print(\"SpaCy language model implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604f9775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T08:58:01.131338Z",
     "start_time": "2021-10-04T08:58:01.122805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spacy_version': '3.1.0',\n",
       " 'location': '/opt/conda/lib/python3.8/site-packages/spacy',\n",
       " 'platform': 'Linux-5.4.0-1018-aws-x86_64-with-glibc2.10',\n",
       " 'python_version': '3.8.6',\n",
       " 'pipelines': {'de_core_news_lg': '3.1.0'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e81a7393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:22:02.881522Z",
     "start_time": "2021-10-04T09:22:02.874891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.6.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk) (2021.9.30)\n"
     ]
    }
   ],
   "source": [
    "# Define UDF for cleaning the raw text (remove special chars and words < 3) with NLTK\n",
    "\n",
    "# Install NLTK\n",
    "!pip install nltk\n",
    "import nltk  \n",
    "from nltk import word_tokenize   \n",
    "\n",
    "def preprocess_nltk(text):\n",
    "    wordlist = nltk.word_tokenize(str(text), language='german')\n",
    "    # punctuation: all special characters, but not sentence endings\n",
    "    punctuation = [',', ';', ':', '(', ')', '[', ']', '{', '}', '\\\"', '\\'','\\'\\'', '\\`', '\\`\\`', \\\n",
    "                   '\\-', '«', '»', '£', '\\^', '~', '*', '®', '•', '■', '♦', '§']\n",
    "    wordlist_stripped = [w for w in wordlist if w not in punctuation]\n",
    "    wordlist_stripped = [w for w in wordlist if len(w) > 2]\n",
    "    wordlist = ' '.join(wordlist_stripped)\n",
    "    return wordlist\n",
    "\n",
    "udf_nltk_tokenize = udf(preprocess_nltk, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ac8af1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:24:52.087167Z",
     "start_time": "2021-10-04T09:24:51.633224Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define UDF for applying large Spacy german model on fulltext to recognize named entites of localities and persons\n",
    "\n",
    "nltk.download('punkt')                # \"punkt\" = standard classifier for sentence segmentation \n",
    "from nltk import sent_tokenize\n",
    "    \n",
    "def spacy_ner_loc(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    ner_loc = []\n",
    "    for s in sents:\n",
    "        doc = nlp(s)\n",
    "        ner_loc.append([ent.lemma_ for ent in doc.ents if ent.label_ == 'LOC'])\n",
    "    return ner_loc\n",
    "\n",
    "udf_spacy_ner_loc = udf(spacy_ner_loc, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349fcb0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-04T09:28:09.131664Z",
     "start_time": "2021-10-04T09:28:09.088043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[contributor: string, coverage: string, creator: string, date: string, description: string, format: string, fulltext: string, id_intern: string, identifier: array<string>, language: string, publisher: string, relation: string, rights: string, source: string, subject: string, title: string, type: array<string>, source_collection: string, locality: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply text cleaning and NER\n",
    "# Use German subset\n",
    "df_nltk = df_cleaned_german.withColumn(\"nltk\", udf_nltk_tokenize(col(\"fulltext\")))\n",
    "\n",
    "# Extract LOC NER with SpaCy\n",
    "df_final = df_nltk.withColumn(\"locality\", udf_spacy_ner_loc(col(\"nltk\"))).drop(\"nltk\")\n",
    "\n",
    "df_final.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "484070cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef clean_locality(text):\\n    #return re.sub(r\\'\\\\[+|\\\\]+\\', \\'\\', text)\\n\\nudf_clean_locality = udf(clean_locality, StringType())\\n\\n# Clean the \\'locality\\' from duplicate \\'[\\' and \\']\\'\\ndf_loc_clean = df_spacy.withColumn(\"locality_clean\", udf_clean_locality(col(\"locality\")))                                    .drop(df_spacy[\\'locality\\'])\\ndf_final = df_loc_clean.withColumn(\"locality\", split(col(\"locality_clean\"), \",\").cast(\"array<int>\"))                                     .drop(df_loc_clean[\\'locality_clean\\'])\\n                                   \\ndf_final.persist()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not in use\n",
    "'''\n",
    "def clean_locality(text):\n",
    "    #return re.sub(r'\\[+|\\]+', '', text)\n",
    "\n",
    "udf_clean_locality = udf(clean_locality, StringType())\n",
    "\n",
    "# Clean the 'locality' from duplicate '[' and ']'\n",
    "df_loc_clean = df_spacy.withColumn(\"locality_clean\", udf_clean_locality(col(\"locality\"))) \\\n",
    "                                   .drop(df_spacy['locality'])\n",
    "df_final = df_loc_clean.withColumn(\"locality\", split(col(\"locality_clean\"), \",\").cast(\"array<int>\")) \\\n",
    "                                    .drop(df_loc_clean['locality_clean'])\n",
    "                                   \n",
    "df_final.persist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4f63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " contributor       | null                 \n",
      " coverage          | Bern                 \n",
      " creator           | [s.n.]               \n",
      " date              | [\"1812\",\"1824\"]      \n",
      " description       | [\"EnthÃ¤lt Druckb... \n",
      " format            | 1 Mappe (10 Druck... \n",
      " fulltext          | null                 \n",
      " id_intern         | 20150342             \n",
      " identifier        | [doi:10.3931/e-ra... \n",
      " language          | German               \n",
      " publisher         | [Haller]             \n",
      " relation          | vignette : https:... \n",
      " rights            | pdm                  \n",
      " source            | null                 \n",
      " subject           | Kaffeehandel         \n",
      " title             | [Kaffee-Verpackun... \n",
      " type              | [Text, Book]         \n",
      " source_collection | E-Rara               \n",
      " locality          | [[]]                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a081b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributor: string (nullable = true)\n",
      " |-- coverage: string (nullable = true)\n",
      " |-- creator: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- format: string (nullable = true)\n",
      " |-- fulltext: string (nullable = true)\n",
      " |-- id_intern: string (nullable = true)\n",
      " |-- identifier: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- rights: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- source_collection: string (nullable = false)\n",
      " |-- locality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1abc5",
   "metadata": {},
   "source": [
    "# Ingest to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a930218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-7.15.0-py2.py3-none-any.whl (378 kB)\n",
      "\u001b[K     |████████████████████████████████| 378 kB 26.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from elasticsearch) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from elasticsearch) (1.26.3)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.15.0\n",
      "Collecting elasticsearch-dsl\n",
      "  Downloading elasticsearch_dsl-7.4.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: elasticsearch<8.0.0,>=7.0.0 in /opt/conda/lib/python3.8/site-packages (from elasticsearch-dsl) (7.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from elasticsearch-dsl) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from elasticsearch-dsl) (2.8.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch-dsl) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from elasticsearch<8.0.0,>=7.0.0->elasticsearch-dsl) (1.26.3)\n",
      "Installing collected packages: elasticsearch-dsl\n",
      "Successfully installed elasticsearch-dsl-7.4.0\n"
     ]
    }
   ],
   "source": [
    "# Install the Elasticsearch APIs\n",
    "!pip install elasticsearch\n",
    "!pip install elasticsearch-dsl\n",
    "import elasticsearch\n",
    "import elasticsearch_dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4117585d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'elasticsearch-1',\n",
       " 'cluster_name': 'docker-cluster',\n",
       " 'cluster_uuid': 'dfUUhrhyTGqWE_tW3ciIMg',\n",
       " 'version': {'number': '7.10.1',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'docker',\n",
       "  'build_hash': '1c34507e66d7db1211f66f3513706fdf548736aa',\n",
       "  'build_date': '2020-12-05T01:00:33.671820Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.7.0',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the basic API with a Elasticsearch client\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\"elasticsearch-1\")\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7545c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explicit mapping for the index\n",
    "# See https://www.elastic.co/guide/en/elasticsearch/reference/7.x/explicit-mapping.html\n",
    "# and https://www.elastic.co/guide/en/elasticsearch/reference/7.x/mapping-params.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecb0d154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tokens\" : [\n",
      "    {\n",
      "      \"token\" : \"dürfte\",\n",
      "      \"start_offset\" : 0,\n",
      "      \"end_offset\" : 6,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 0\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"von\",\n",
      "      \"start_offset\" : 7,\n",
      "      \"end_offset\" : 10,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 1\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"anfang\",\n",
      "      \"start_offset\" : 11,\n",
      "      \"end_offset\" : 17,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 2\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"ein\",\n",
      "      \"start_offset\" : 19,\n",
      "      \"end_offset\" : 22,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 3\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"anderer\",\n",
      "      \"start_offset\" : 23,\n",
      "      \"end_offset\" : 30,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 4\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"hinweis\",\n",
      "      \"start_offset\" : 31,\n",
      "      \"end_offset\" : 38,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 5\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"als\",\n",
      "      \"start_offset\" : 39,\n",
      "      \"end_offset\" : 42,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 6\n",
      "    },\n",
      "    {\n",
      "      \"token\" : \"selbstverständlich\",\n",
      "      \"start_offset\" : 43,\n",
      "      \"end_offset\" : 61,\n",
      "      \"type\" : \"<ALPHANUM>\",\n",
      "      \"position\" : 7\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1271  100  1154  100   117   160k  16714 --:--:-- --:--:-- --:--:--  177k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Test standard analyzer\n",
    "# Use private IP\n",
    "curl -X POST \"http://172.26.10.176:9200/_analyze?pretty\" -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"analyzer\": \"standard\",\n",
    "  \"text\": \"D\\u00fcrfte von Anfang. Ein anderer Hinweis als\\nselbstverst\\u00e4ndlich.\"\n",
    "}\n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8bc18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest to Elasticsearch as JSON\n",
    "# https://www.elastic.co/guide/en/elasticsearch/hadoop/7.x/spark.html\n",
    "# See settings https://www.elastic.co/guide/en/elasticsearch/hadoop/7.x/configuration.html\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "esconf={}\n",
    "esconf[\"es.resource\"] = \"index/erara\"         # format: index/type\n",
    "esconf[\"es.mapping.id\"] = \"id_intern\"         # if 'id_intern' should be used as global ID in ES -> updating!\n",
    "esconf[\"es.nodes\"] = \"elasticsearch-1\"\n",
    "# OR\n",
    "#esconf[\"es.nodes\"] = IP                        # default: 'localhost'\n",
    "#esconf[\"es.port\"] = \"9200\" \n",
    "\n",
    "#esconf[\"es.mapping.include\"] = None             # fields to include with ingest\n",
    "#esconf[\"es.mapping.exclude\"] = None             # fields to exclude with ingest\n",
    "\n",
    "#esconf[\"es.read.field.as.array.exclude\"] (default empty)   # Fields/properties that should NOT be considered as arrays/lists\n",
    "#esconf[\"es.read.field.as.array.include\"] = nested.bar:3    # maps nested.bar as a 3-level/dimensional arra\n",
    "\n",
    "esconf[\"es.write.operation\"] = \"upsert\"\n",
    "#index (default): new data is added while existing data (based on its id) is replaced (reindexed). \n",
    "#create: adds new data - if the data already exists (based on its id), an exception is thrown. \n",
    "#update: updates existing data (based on its id). If no data is found, an exception is thrown. \n",
    "#upsert: known as merge or insert if the data does not exist, updates if the data exists (based on its id). \n",
    "\n",
    "df_final.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "                    .options(**esconf).save()     #for adding fields: .mode('append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1b698773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short form\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "df_final.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.resource\", \"index/eperiodica\") \\\n",
    "    .option(\"es.mapping.id\", \"id_intern\") \\\n",
    "    .option(\"es.write.operation\", \"index\") \\\n",
    "    .option(\"es.nodes\", \"elasticsearch-1\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd3efe",
   "metadata": {},
   "source": [
    "# Query Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935c9099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_name': 'docker-cluster',\n",
       " 'status': 'yellow',\n",
       " 'timed_out': False,\n",
       " 'number_of_nodes': 1,\n",
       " 'number_of_data_nodes': 1,\n",
       " 'active_primary_shards': 7,\n",
       " 'active_shards': 7,\n",
       " 'relocating_shards': 0,\n",
       " 'initializing_shards': 0,\n",
       " 'unassigned_shards': 1,\n",
       " 'delayed_unassigned_shards': 0,\n",
       " 'number_of_pending_tasks': 0,\n",
       " 'number_of_in_flight_fetch': 0,\n",
       " 'task_max_waiting_in_queue_millis': 0,\n",
       " 'active_shards_percent_as_number': 87.5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.cluster.health(wait_for_status='yellow', request_timeout=1)   # 6 shards are default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0edde4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2324  100  2324    0     0  1134k      0 --:--:-- --:--:-- --:--:-- 1134k\n"
     ]
    }
   ],
   "source": [
    "# Whole mapping of index\n",
    "! curl -X GET \"http://172.26.6.171:9200/index_1/_mapping?pretty\" > mapping_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941c2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search on field of index + type via REST API\n",
    "# Use private IP\n",
    "\n",
    "#! curl -GET \"http://172.26.6.171:9200/index_1/eperiodica/_search?q=title:aare&pretty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f3035d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response: [<Hit(index_1/pB-tT3wBsgv0SeAE1D_2): {'creator': 'Singeisen, Raphael', 'date': '2018', 'format': ...}>]> \n",
      "\n",
      "Aus Wald wird Ackerland : Kriegsrodungen im Kanton Bern 1941-1946 : ein umstrittenes Kapitel der Anbauschlacht \n",
      "\n",
      " Historischer Verein des Kantons Bern\n"
     ]
    }
   ],
   "source": [
    "# Simple search via DSL API\n",
    "from elasticsearch_dsl import Search\n",
    "\n",
    "s = Search().using(es).query(\"match\", publisher=\"verein\")    # fulltext=\"bern\"\n",
    "response = s.execute()\n",
    "print(response, \"\\n\")\n",
    "for hit in s:\n",
    "    print(hit.title, '\\n\\n', hit.publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b25da469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 213 hits found.\n",
      "/index_1/eperiodica/hB-tT3wBsgv0SeAE1D-A returned with score 2.094948\n"
     ]
    }
   ],
   "source": [
    "s = Search()\\\n",
    "        .using(es)\\\n",
    "        .query(\"match\", fulltext=\"frau\")\\\n",
    "        .extra(track_total_hits=True)\n",
    "\n",
    "response = s.execute()\n",
    "print('Total %d hits found.' % response.hits.total.value)\n",
    "\n",
    "h = response.hits[0]\n",
    "print('/%s/%s/%s returned with score %f' % (\n",
    "    h.meta.index, h.meta.doc_type, h.meta.id, h.meta.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9fdffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "6\n",
      "eq\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "print(response.success())\n",
    "print(response.took)\n",
    "print(response.hits.total.relation)\n",
    "print(response.hits.total.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "136c5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 'index_1', 'id': 'hB-tT3wBsgv0SeAE1D-A', 'score': ...}\n",
      "2.0949483\n",
      "Fremder Besuch im Oberland um 1810 : kleine Hörszene von Christian Lerch, gesendet von Radio Bern im Mai 1940\n",
      "Lerch, Christian\n",
      "zgh-001:1944:6::275\n",
      "[[Pfarrhaus Lauterbrunnen, Ahlfeld, Bern, Kuli, Schweizerisch, Ahlf, you know, Europa, Schweiz, Schweiz, Ahlf, Seje cha nid, Lippe, Überhoupt, Staubbach, Staubbach, Berg, Ahlf, begei¬, Ahlf, Scotland, Ahlf, ge¬, Ahlf Huuh, Ahlf, Bergabgehen, Ahlf, Ahlf, Oberland, Hörszene]]\n"
     ]
    }
   ],
   "source": [
    "for hit in s[0]:\n",
    "    print(hit.meta)\n",
    "    print(hit.meta.score)\n",
    "    print(hit.title)\n",
    "    print(hit.creator)\n",
    "    print(hit.id_intern)\n",
    "    print(hit.localities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a491d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Zwangsanleihen Massenas : ein Bild aus dunkler Zeit\n",
      "[[Frankreich, Frankreich, Schweiz, Frankreich, Glarus, Gotthard, Zürich, Österreicher, Stadt, Russe, Österreicher, Waldstätten, Linth, Paris, Stadt Zürich, Zürich, Frankreich, Massena, Stadt Zürich, St. Galle, Zürich, Bürgerschaft, Zürich, Stadt, Zürich, Zürich, Stadt Zürich, Basel, Stadt Basel, Stadt Basel, Stadt Winterthur, Winterthur, Basel, Frankreich, Zürich, Winterthur, Basel, Bern, Schweiz, Frankreich, Binningen, Basler, Stadt, Stadt Basel, Helvetiens, Basel, Stadt Basel, Basel]]\n",
      "---\n",
      "Der Hafner Heinrich Hess uns sein Hand- und Hausbuch : ein Beitrag zur Geschichte der stadtbernischen Hafnerei am Ende des 17. Jahrhunderts. Teil 2\n",
      "[[Biel, Basel, Johannesen, Zürich, Zürich, Züricher, Zürich, Zürich, Zollikon Zürich, Winterthur, Schiben, Reutlinger, Jegenstorf, Winterthur, gsin, gsin, Brugg, Jegenstorf, Schneckenhäfeli, bruni, gsin, Weinachten, Burgdorf, Burdlofer Heiri, Lichtmeß, Lichtmeß, Giben, Burgdorf, Burgdorf, Badhaus, Basel, Genf, gsin, Jegenstorf, Bern, Niclaus]]\n",
      "---\n",
      "Das Selbstbewusstsein des Bauern\n",
      "[[Schweiz, Hautpsache, Hirtentum, innern Schweiz, Emmental, Kühertums Emmental, Schweiz, Mittellandes, Luzernbietes, Emmental, Berner, Emmentaler, Republik Bern, Bürgerschaft, Europa, Berner, Europa, Berner, Berner, Bern, Zürich, Basel, Agrarrevolution, Bern, Europa, Land, Stadt, Bern, Stadt Bern, Stadt Bern, Berner, Zürich, Berner, Republik, Waldmannschen Spruchbriefe, Stadt Winterthur, Rebbauern See, citoyens, Agrarrevolution, Bern, Katzenrütihof, Rümlang, Bern, Stadt, Bern, Eigenhöfen, Berner, Stadtzürcher, Staat Bern, Staatsinteressen, Kanton Zürich, Zürichsees, Zürcher Oberland, Schweiz, Kanton Zürich, Schweiz, Schweizervolk, Schweiz, Bund, Kanton Bern, Kanton Zürich, Stadt Zürich, Stadt Winterthur, Zürich, Zürich, Winterthur, Kanton Zürich, Bayer, Oberbayern, Zürich, Kanton Zürich, Schweiz, Kanton Zürich, Mißlingen, Alpe, Rom]]\n",
      "---\n",
      "Erinnerungen an Dr. Friedrich Emil Welti und Frau Helene Welti-Kammerer\n",
      "[[Berner, Berner Stadtrechts, Murtens, Berner, Bern, Zurzach, Winterthur, Berner, Florenz, Bern, Rom, Florenz, Biel, Rom, Steffisburg, Berner, Rattenholz, Amt Seftigen, Rom, Merligen, Italien, Rom, Bund, Bern]]\n",
      "---\n",
      "Eingriffe ins Berner Stadtbild seit hundert Jahren\n",
      "[[Stadt, Stadt, Altstadt, Nydegg, Bern, Bern, Hundertfünfzig, Stadt Bern, Erde, Altstadt, Stadt, Bern, Rathauses, Heiliggeistkirche, Münster, Landkirchen, Stadtgesichts, Europa, Platzgürtel, Käfigturm, Bern, Aare, Altstadt, Aare, Bern, Freiburg Üchtland, Bremgarten Aargau, Zürich, Altstadt, Brückenbögen, Stadtinnern, Kramgasse, Stadtinnern, Altstadt, Altstadt, Altstadt, Haus für Haus und Hof für Hof, Altstadt, Zeitglockens, Bern, Nydeggbrücke, Hochstraße, Reichsburg, Altstadthäuser, Altstadt, Freiburg der Fall, Bundesstadt, Untertorbrücke, Felsenburg, Christoffelturm, Bern, Berner, Stadt, Stadt, Stadt, Rathauses, Christoffel- Schwanen- und Bundesgasse, Bern, Kirchenfeldbrücke, Inselspital, Bundeshauses Ost, Hotel Bellevue, Bundesratshaus, Bundeshaus West, Florenz, München, Athen, Bemühend, Bedenken, Turmausbaues, Parlamentsgebäudes, Barfüßerklosters, Hotelgasse, Christoffelturm, Stadt, Kunstwertes, Hauptwache, Bern, Landjägerwache, Schweiz, Bern, Kramgasse, der Schweiz, Alte Museum, Landjägerwache, Alte Museum, Hauptwache, Hauptwache, Kasinoplatzes, Hauptwache, Hauptwache, Theaterplatzes, Stadt, Hauptwache, Eckhäuser, Café Théâtre, Theaterplatz, Hauptwache, Winterthur, Hauptwache, Theaterplatz, Verbindungsbögen Bücken, Nydeggbrücke, Itiseli, Christoffelturm, I\\U, Bundeshauses Ost, Bellevue mMmwmmfam, Hotel Bellevue Palace, TAFEL •fe- Münsterturm, Historisches Museum, Hotelgasse, Hauptwache, Hauptwache, Winterthur, Kramgasse, Münster, Badgasse, Badgasse, Plattform, Münster, Altstadt, Jennerhaus Kasinoplatz, Hauptwache, Winterthur, Kasinoplatz, Jennerhauses, Altstadt, Hauptwachedrama, Land, Bückgrat, Altstadt, Kramgasse, Taf VIII, Kammermusiksaales, Fensterlosigkeit Innern der Laub, Haus der der Musen, Bauflucht, Stadt, Kramgasse, Bern, Rathauses, Rathauses, Rathaus, Rathaus, Kornhauses, Fässadenerneuerungen Münsterplatz, Herrengasse, Münsterplatz, Abdeck- platt, Kirchenfeldbrücke, Badgasse, Murifeld, Lorraine, Freiburg, Altstadt, Nydeggbriickenkopf, Altstadt, Altstadt, Altstadt, Nydegghof, Mattenenge Stalden, Nydeggkirche, Läuferplatz, Bern, Nydegg, Mattenenge, Aargauerstalden, Rosengarten, Stadbild, Gemeindeabstimmung, Berner]]\n",
      "---\n",
      "Das Jahrzeitenbuch des Heiliggeistklosters in Bern\n",
      "[[BERN, Heiliggeistkloster, St.-Vinzenzius-Münsters, Heiliggeistkloster, Bern, Bistum Lausanne erwähnt7, Montpellier, Santa Maria Sassia Rom, Berner, Lausanne, Berner Gemeinschaft, Lutfridus, Heiliggeistordens, begonnen18, Spitals Stephansfeld Elsaß, Bern, an20, Bern, Ordenshäuser, Berner, ORDIS.SCI SPC.22 Wann, Berner, Berner Kloster, Lausanne, Burgundia, Stadt Bern, Spital, Oberes, Altes Spital, Ordensspitals, Alte Spitals, Berne, nieder, Heiliggeistspitals, Stadt, Bern, Bern, Memminger, Stadt, Berner, Stadt Bern, Jahrzeiten, Stadt, Kloster, Verwaltungsbücher, Stadt, Winterthurer Fund, Pergamenthülle, Heiliggeistbrüdern, Vorläuferbuch37, Scharnachtals, tholome, ober Spital, Jahrzeitenbuch, ober Spitals, Jahrzeitenbuch, Vorläufer-Anniversar, Heiliggeistkirche, iärlicher, Rom, Amptzennberg Hennßlis, Wabernn45, Jagkispach, Bern, brudertüch49, vigilg, sol, cruitz, kertzen, kinden, wuchenbrieff, zinß, sant Anthonien, neywer uff stat54, bruoderduoch, Inselfrowen, schwands, vigil, messen, Schowlitzengassen, Schowlitzengassen, Fontes Rerum, Schweiz, Winterthur, Berner, Berner, Winterthur, Berner, Burgerspitals, Bern, Burgerspitals, Stadt Bern, Bern, Stadt, Burgerspitals, Helvetia, Lausanne, Stuttgart, Steynitz, Berlin, Rom, L'Ospedale Santo Spirito Saxia, Berner Haus, Berner, Stadt Straßburg Straßburg, Bern, Berner, Berner, Straßburger Bürgerspitals], [Wien, Zürich, Les monuments, Neuchâtel ville Neuchâtel, Neuenburger Haus, Besançon, Neuchâtel, Saint-Jacques Besançon Musée neuchâtelois, Spital, Reicke, Burgerspitals, Bern, Bern, Hensli, Wabern, Hensli, Wabern, Bern, Villarzel, spital zwen, Jagispach, Stadt Bern, Bern, Archer, ober Spital, Burgerspitals, Bern, Jahrzeit, Bern, Bern, Brudertuch, Wien, Bern, Bern, Hannover, Bern, Burgerspitals, Bern, Kanton Bern, Burgerspitals, Bern, Bern]]\n",
      "---\n",
      "Lavaters Aufzeichnungen über seine Berner Reise : ein Beitrag zur Haller-Forschung\n",
      "[[Bern, Bern, wobey, Leipzig, Winterthur, Bern Mss Hist Helv XVIII, Zürich, Tübingen, Gemmingen schrieb9, Schmeicheley, Bern, Frauenfeld, Bern, Bern, Berner, Zürich, Bern, Berner, Bern, Berner, Bern, Berner, Bern, Schloß Baden, Baden, Mellingen, Mägen-, Mägenwil, Freyämtern, dannLenzburg, Lenzburg, Schulth Hünerwadels seynes, Lenzburg, Aarau, Köllikon Kölliken, Muchen Muhen, Baden, Schinznach, Baden, Schalkheit, Neuchâtel, Herzogenbusch Herzogenbuchsee, Durchreisende, Wirtshaus Morgenthal, Bern, Schinznach, Bern, Kilchberg, Stein, Thurm Zürich, Jaquesdreau, Essen, Münster, Zürich, Essen, Bern, Zürich, Olthof, Bieler, Neuenburger See, Stadt, Erschütterungsbedürftigkeit, Bern, geessen, Laubdach, Vögelgepfeif, Caffé, Neuenburg, Ziseln Siselen, Fürstentum St. Galle, Zihlbrugge, Bern, Bern, Zürich, Laub Impertinenz, Berner]]\n",
      "---\n",
      "Rudolf Abraham Schiferli 1775-1837 : eine biographische Skizze\n",
      "[[Basel, Österreicher, Grenzach, Rhein, Bern, Berner, Waadt, Schiferli, Berner Laub, Schiferli, Thuner, Schiferli, Ammerswil, Villmergen, Bern, Bern, Jena, Zeit Jena, Berner, Wien, Österreich, Wien, Paris, Pariser, Berner, Thun, Zürich, Bern, Bern, Zähringerstadt, Paris, Bern, Schiferli, Österreich, Rorschach, Legionschirurg Majorsrang April, Schiferli St. Galle, Feldsanitätsdienstes, Österreicher, Eglisau, Winterthur, Aarau, Stadt, Spital Thun, Berner, Oberchirurgen, Bern, Solothurn, Aarau, Österreicher, Zürich, Münchenbuchsee, Militärspital, Schiferli, Bern, Schiferli, Freiburg, Schiferli, Lausanne, Baden, Oberchirurg, Schwyz, Bern, Schiferli, Kanton Bern, Genf, Schiferli, Bern, Berner, Berner, Belgien, Berner Stadtbibliothek, Adelsdiplom, Schiferli, Thun, Elfenau, Berner, Sanitätswesens, Leid, Republik, Schiferli, Militärspitälern, Österreich, Frankreich, Bern, Schiferli, Lausanne, Sanitätswesens, Schiferlis, Schweiz, Frankreich, Verwaltungsfachleute, Spitalpolizei, Schweiz, Bedeutng, Buchsee, Schweiz, Daviel, Nürnberger, Bern, Schweiz, Militär-Sanitätswesen, Stadtbibliothek Bern, Beatusstraße, Bern, Bern, Fribourg, Schweiz, Quellenstudie, Lausanne, Bern]]\n",
      "---\n",
      "Briefe Karl Ludwig von Hallers an seinen zürcher Grossvater, Hans Caspar Schulthess, aus den Jahren 1782-1797\n",
      "[[Aare, Ämterlaufbahn, Zürich, Haller, Zentralbibliothek Zürich, Zürich, Nachfahrentafeln, Basel, Berne, Zurich, Zürich, Waldli, Zürich, comprendre les auteurs, Bern, Nyon, Marburg, Solothurn, prochain les fréquenterai toujours, qui est aux bains, ville Genève, Zürich, Hottinger- straße, unter Freier, Rumilly petite ville très sale, bains salutaires qui nourrit der étrangers fait aussi soie l'année, Piémont Les bains, qu'un deux équipages, Lyon, Chambéry qui n'est, Aix, lac d'Annecy dans pays fertile matin nous partîmes d'Annecy passâmes, Les seuls, bâtimens, Lac Joux sur Dent Vau lion ville d'Yverdon, Neutralitätspartei, être égales par, Nyon, Landvogt Rheintal, Straßburg, Deutschland, Holland, Ämterlaufbahn, Stadt, Augsburg, Bern Genealog, Straßburg, Erlach, Welschseckelmeister Schultheiß, Bern, St. Hubertus-, Bern, Lac Joux Yverdon, nerfs surtout, bains lac comme devrais, Lac Joux long, Vous saurez depuis, Bern, Stadt, Bern, Moudon, Paris, moraux, Marseille, Paris, Italien, Rom, Kirchenstaates, Paris, Bern, Preuße, Prag, Neuenburg, Bern, Köniz, Bern, Sinner Bal ai-, autrefois, bibliothécaire célèbre par ses ouvrages est mort ces jours passés, Bern, Freistaates Bern, Bern, Berner Bibliothek, Bern, manière que trouve, Hausen Baden, Winterthur, Bex, Zürich, Bern, Rastatter, Solothurn, Wyß, Solothurn, Frankreich, collecte, Bern, ces Louis les intérêts que vous dois afin que notre compte, Pays Gex, Genf, Zürich, Berner Regierung, Genf, Mailand, Paris, Rastatt, nous fait beaucoup plaisir]]\n",
      "---\n",
      "Samuel Studer (1757-1834)\n",
      "[[Naturhistor, Naturhistorischen Museum Bern, Bernische Naturhistorische Museum, Bern, Stadt Bern, Unteraargletscher, Naturkunde Helvetiens, zubrachte1, Bern, Münster, Grafenried bei Fraubrunnen, Sonnseite, Herrengasse, Burgerspital, Büren, Bern, Schweiz, Heiliggeistkirche, British Museum, Stettlen, Bolligenstraße, Lieblingsstudien, Bern, Tralles, Lausanne, Bern, Europa, Land, Staat, Europa, Land nuzlich seyn, Bern, Bern, Egg Thierachern, Berner Alpe, Studer, Thierachern, Egggutes von Thierachern, Alpe, Bern, Terrasse Thierachern, Äschi, Gasthofs zum Löwe, Bern, Thierachern, Bern, Interlaken, Wyttenbach, Biel, Bern, Stockhorn, Niesen, Thuner, Schweiz, Berner, Schweizerlandes, Oberlandes, Grönland, Schweiz, Berghinan, Oberlandes, Wallis, Wyttenbach, Chamonix, Finsteraarhorn, Bern, Berner Alpe, Festung Aarburg, Zwölfmal, überquert31, Berner, Wallis, Münster, Breslau, Berner, Berner, Bern, Büren, Bern, Köniz, Burgerspital, Barometer-, Thermometerstandes, Kirchlindach, Calabrien, Sicilien, Erdbeben Prof. Rudolf Wolf, Alpe, Alpe, Berner, Berner, Molluskenfauna, Naturhistorischen Museum Basel, Murtensees, Zeitvertrieb, Striée, Berner, Vorwelt, Malakologen, Schweiz, Bex, Schweiz, Nachtschnecke, Heinzmanns, Stadt, Republik Bern, Description ville Berne, naturforsehender, Bern, Kreis, Wyttenbach, Wyttenbach, Wyttenbach, Bern, Stadt, Erde, Jupiter, Laubfröschen, Val, Frankreich, Bern, Europa, Stadt, Berner, Bern, Bern, Naturhistorischen Museum, Wyttenbach, Land, See-Einhornes, Südsee-Entdeckungsreise, Sprüngli, Berner, Naturalienkabinetts Wien, Lämmergeier, Wien, Berner Galerie, Dorpat, Livland, Bernern, Ostsee, Bern, Berner, Bernischen Naturhistorischen Museum, Erlenbach, Naturhistorische Museum, Museum Bern, Bern, Zeit Zeit, wobey, Genf, Zürich, Hanau, Universität Erlangen, Büren, Gingins, Neuernannten, Gültbriefkommission, Kirchenguts, Curatel, Wyttenbach, Bern, Wange, Stadt Bern, Europa, Göttingen, Bern, Utzenstorf, Schweiz, Interlaken, Kirchenvermögens, Kirchenguts, Kanton Bern, Unpartheiische, Bern, Mutach, Berner Münster, Bern, Utzenstorf, Bern, Oncle, Berner, Münster, Bern, Bern, Paris, Berner, Mutach, Wange, Berner Naturfreunde, Klosterhof, Kirchenfelds, Berner Alpe, Bern, Land, Grafenried bei Fraubrunnen, Bern, Schweiz, Frankreich, Italien, hahen, Genf, Basel, Frankreich, Italien, Berner, Entlebuch, Luzern, Stanz, Engelberg, Scheidek Grindelwald, Lauterbrunnen, Unterseen, Thun, Bern, Gelehrtenzweig Basel, Bern, Spitalgasse, Schweiz, Panoramenzeichner, Panoramenm, Bernischen Naturhistorischen Museum, Bern, Bern, Kreis, Naturhistorischen Museum, Naturhistorischen Museum Bern, Nyon, Schwarzenburg, St. Bern, St. Bern Vgl, Bern, Berner Bibliothek érudit, Wyttenbach, Bern, Mss XVI Vgl, Stadtbibliothek Thun, Sulgenbach, Bern, Winterthur, Stadt], [Bern, Bern, Bern, Wyttenbach, Bern, Zürich, Berner, Naturhistorischen Museum, Bern, St. Bern, St. Bern SRM, St. Bern Allg Schweizerzeitung, Bern, Bern, Republik Bern, Vgl, Gotthelfs, Utzenstorf, Schloß Landshut, Bern, Allg Schweizerzeitung, Bern, Curatel, Mss XVII, Bern, Bern, Bern, Bern, Bern, Zürich, Bern, Naturkunde Helvetiens, Bern, Schweiz, London, Stadtbibliothek Bern, Stadt Bern, Bern, Quid ecclesia Quis, Alpe, Bern, Bern, Bern, Stadtbibliothek Bern, Zürich, Bern, Stadtbibliothek Bern, Bern, Bern, Säcular-Predigt, Bern, Kirchenguts, III Zürich, Zürich, Bern, Bern, Berner Bibliothek, Berner, Bern, Bern, Bern, Stadtbibliothek Bern, Bern, Bern, Naturhistorische Museum Bern Galenica Nov, Bern, Genève, Lyon, Berner, Bern, Bern, Berner, Basel, Bern, Leipzig, Bern, Bern, Bern, Bern, Bern, Bern, Bern, Bern, Bern, Leipzig, 144—167 Mfeisner Friedrich, Museum der vaterländisch Naturgeschichte, Bern, Bern, Leipzig, Bernische, Zürich, Berner, Berner, Bern, Bern, Allgemeine Schweizerzeitung, Aarau, Bern, Winterthur, Berner, Studer-Apotheke, Bern, Berner Oberlandes, Bern, Berner, Zürich, Bern, Chamonix, Kiental, Kandersteg, 1781—1789, Burgerspital, Bern, Grindelwald Oberhasli, Grimsel, Unteraargletscher, Pyrenäen, Frutigamt, Gasterntal, Bern, Kirchenguts, Herzogenbuchsee, Universität Erlangen, Entlebuch Luzern, Engelberg, Oberhasli, Große Scheidegg, Thun, Wallis, Domodossola, Nufenenpaß, Grimsel, Bern, Bern, Rudoll, Bern, Grafenried, Egg Thierachern, Bern, Büren a.d, botanisch Garten, Thun, Bern, Heiliggeistkirche, Großrat, Naturmst, Bern]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "s = Search().using(es)\\\n",
    "        .query(\"multi_match\", query='winterthur', fields=['title', 'localities'])\n",
    "#s = s.extra(explain=True)\n",
    "\n",
    "response = s.execute()\n",
    "for hit in s:\n",
    "    print(hit.title)\n",
    "    print(hit.localities)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efe8a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search Elasticsearch API via \"scan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a232875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Die Apotheker der Stadt Biel : Vortrag gehalten an der Jahresversammlung des Kant. Apothekervereins in St. Immer, 10. November 1957',\n",
       "  \"[[Schweiz, Basel, Apothekenwesen, Biel, Schweiz, Biel, Apothekerwesens, Stadtarchivs, Säßhaus, Graubünden, Hintersäßen, Biel, Bieler, Bern, Bieler, Biel, nisgeld, Wyttenbach, Bern, Stadt Bern, Corallen-, Nördlingen, Biel, Biel, Berner, Biel, Biel, Land, Bieler, Venneramt, Armengutes, Darmstadt, Ritter'sche Apotheke, Burgplatz, Stadt, Chorgerichtshandels, Provisorenstelle, Eckhaus Ring, Franzose, Biel, Haus der obern Apotheke, Biel, Bern, Windsheim, Biel, Bern, Bartolomäus Knecht, Haus der Bäcker Müller der Schmiedengasse, Bacharach, Nidau, Biel, Bern, Bern, Knecht'schen Apotheke Bern, Leipzig, Provisorenstellung, Bern, Haus der Schmiedengasse, Willstädt, Heidenheim, Biel, Basel, Brotschal, Lausanne, Württemberger, Glarus, Bern, Brennkirschen, England, Stadt, Frankreich, Biel, Biel, Basel, Neuenburg, Biel, Zürich, Jurahöhen, Orbe, Lausanne, Genf, Jurabergen Biel, Chasserai, Tessenberg, Biel, Stadt, Biel, französische Herrschaft, Bezirk Delémont, Kanton Biel, les formes voulues par règlement Cette cause est rigueur vous recommande surplus une surveillance sévère sur, Biel, Militärspital, Stadt, Bern]]\"],\n",
       " ['Der Tempelbezirk Patinesca bei Biel : Ausgrabungen 1937-1939',\n",
       "  '[[Petinesca, Studenberg, Biel, Petinesca, Aare, Zihl, Bodensee, Genfersee, Aare, Jura, Augusta Raurica, Umgangstempel, Weihende, Weihegaben, Steindenkmälern, Jupiter, Juno, Kapitol Rom, Reiches, Mars, Wesltor, Umgangstempeln, Inder, Torturmanlage, Patinesca, Biel]]'],\n",
       " ['Bern und Biel als Schiedsrichter in den Freibergen : Vermittlung wegen eines 1508 verübten Totschlags',\n",
       "  '[[Pruntrut, Fürstbistums Basel, Schweiz, Bern, Biel, Basel, Bern, Tessenberg, Münstertals, Burgrecht, Bern, Biel, nicht9, Hochgerichtsherrn, Fürstbistum Basel, Reich, Basler, Warmole, Frîenberg, Nidau, Biel, Biel, Basel, Santursitz17, Landtsbruch, Ire, Heren, Bern, Biel, Basel, Bern, Biel, Basel, Liecht, Ansechung24, Frienberg, Basel, Ire, Urfecht, sin Bruder, hinfür, dwil, Basel, Ziten, Basel, Zugewandten, verendern, Stifft, Rechten39, benügen, Stifft, Basel, Urfecht, Bern, Biel, harzü hencken, Bern, erbarn, Sant Ursitz, Bern, Biel, Gnod, Heyligen, Landgerichts uff, geschworn, Stûl Rom, bern, Berner, Saignelégier, Liben, loßen, Bern, Biel, Landgericht, Bern, Biel, Bern, Biel]]']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple search via Elasticsearch API  and 'helpers' module\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "search = scan(es,\n",
    "    query={\"query\": {\"match\": {\"title\": \"biel\"}}},\n",
    "    index=\"index_1\"         \n",
    ")\n",
    "[[i['_source']['title'], i['_source']['localities']] for i in search]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "E-rara data transformation with Spark and ingest to Elasticsearch",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
